runner:
  n_epochs: -1
  #n_epochs: -1
  total_steps: 1941300
  #total_steps: -1
  gradient_clipping: 5.0
  gradient_accumulate_steps: 4

  log_step: 10000
  save_step: 10000
  max_keep: 100

  fp16: false

optimizer:
  name: AdamW_with_schedule
  lr: 2.e-4
  warmup_proportion: 0.07

pretrain_expert:
  datarc:
    num_workers: 16
    train_batch_size: 32
    max_timestep: -200 # Max length for audio feature (0 for no restriction, negative value to set minimum timestep)
    #libri_root: '/media/andi611/1TBSSD/LibriSpeech/' # If raw libri data is provided, use on-the-fly feature extraction, else use the pre-extracted features under `file_path`
    libri_root: '/home/kaen2891/workspace/ssl_ver2/'
    #file_path: 'data/len_for_bucket' # Pre-extracted features path. When using on-the-fly feature extraction, this is used to provide length for bucketing.
    file_path: '/home/kaen2891/workspace/asr2/s3prl/s3prl/preprocess/data_len_for_bucket/ssl_ver2_len_for_bucket'
    #sets: ['train-clean-100', 'train-clean-360', 'train-other-500'] # can be the subset of ['train-clean-100', 'train-clean-360', 'train-other-500']
    #sets: ['KoSpeech_1000hour', 'NIKL_SEOUL', 'command_sentence_adult', 'command_sentence_child', 'command_sentence_elder', 'foreign_sentence', 'free_sentence_adult', 'free_sentence_child', 'free_sentence_elder']
    sets: ['foreign_sentence']
